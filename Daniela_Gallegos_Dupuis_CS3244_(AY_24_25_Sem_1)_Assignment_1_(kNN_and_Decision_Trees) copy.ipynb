{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2cfb07e8",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danigallegdup/ACM-ICPC-Algorithms/blob/master/Daniela_Gallegos_Dupuis_CS3244_(AY_24_25_Sem_1)_Assignment_1_(kNN_and_Decision_Trees).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YMulSXCie5d5",
      "metadata": {
        "id": "YMulSXCie5d5"
      },
      "source": [
        "Available at [Canvas](https://https://www.nus.edu.sg/canvas/) \"CS3244/Assignments/Assignment 1 (Individual)\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5-8Ih5dge7VO",
      "metadata": {
        "id": "5-8Ih5dge7VO"
      },
      "source": [
        "---\n",
        "\n",
        "**Make sure you reserve sufficient time to upload your exported copy to Canvas.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TZIaecT9uF6r",
      "metadata": {
        "id": "TZIaecT9uF6r"
      },
      "source": [
        "# Assignment\n",
        "\n",
        "We have introduced $k$**-Nearest Neighbors** and **Decision Trees** in the lectures on Weeks 02 and 03. In this **individual** assignment, you will be graded on implementing the models yourself. You will run both models and compare their performance on the same dataset. Overall, you will learn to appreciate the similarities and differences in models to predict on the same problem."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hFe14MROoltp",
      "metadata": {
        "id": "hFe14MROoltp"
      },
      "source": [
        "## Assignment Instructions\n",
        "\n",
        "Before the assignment, you should **create a copy** of this Colab file in your own Google Drive.\n",
        "\n",
        "In this assignment, you will\n",
        "\n",
        "1. Follow the instructions to familiarize yourself with the `wine` dataset;\n",
        "2. Write your code in the designated spaces to finish the implementations of the $k$-Nearest Neighbor and Decision Tree algorithms;\n",
        "3. Run the notebook to obtain your results;\n",
        "4. Upload this Colab file into Canvas \"CS3244/Assignments/Assignment 1 (Individual)\"\n",
        "\n",
        "**Assignment Deadline: September 8th (Sunday) 23:59 SGT**  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a75b5185-8cb4-49c3-978f-7c234bee90b8",
      "metadata": {
        "id": "a75b5185-8cb4-49c3-978f-7c234bee90b8"
      },
      "source": [
        "## 1. Programming: $k$-NN and Decision Tree from `sklearn`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24e5f97a-beff-4773-b90c-55ec53eec6e0",
      "metadata": {
        "id": "24e5f97a-beff-4773-b90c-55ec53eec6e0"
      },
      "source": [
        "### .a Loading and Visualizing Input data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59fcb9eb-aa7d-43db-9378-8af8983eec88",
      "metadata": {
        "id": "59fcb9eb-aa7d-43db-9378-8af8983eec88"
      },
      "source": [
        "\n",
        "We'll use the [Wine](https://archive.ics.uci.edu/ml/datasets/Wine) dataset from the popular [UCI dataset repository](https://archive.ics.uci.edu/ml/index.php).  This describes the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 chemical constituents found in each of the three types of wines.\n",
        "\n",
        "We'll load in the data for white wines, take a look around and split the data into parts for training the model and testing it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ebed0d46-50bd-447d-9292-256bfacab8cd",
      "metadata": {
        "id": "ebed0d46-50bd-447d-9292-256bfacab8cd"
      },
      "outputs": [],
      "source": [
        "# Import the standard tools for pythonic data analysis.\n",
        "import csv\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Let's read the data in as a \"data frame\" (df), equivalent to our D = (X,y) data matrix\n",
        "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv',sep=';') # Separate on semicolons"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6af47084-cf5c-44f7-9c7a-dc7b59b0decb",
      "metadata": {
        "id": "6af47084-cf5c-44f7-9c7a-dc7b59b0decb"
      },
      "source": [
        "We want to apply matchine learning algorithms to predict the quality of the wine from its constituents. That is, we will utilize algorithms that can find the correlation between wine quality and its 13 constituent (as features) to make prediction. To better understand how algorithms can achieve this, we need to inspect the data distribution first.\n",
        "<!-- (actually, we can predict any feature from any other feature, there's really no particular distinction between $\\mathbf{x}$ and $y$).   -->\n",
        "Let's take a look at the distribution for **quality**. This tells us the how the wine quality is distributed without knowing anything about other information. Go ahead and run the below cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "778831d9-de53-4da4-8ade-e3dcbd2922b8",
      "metadata": {
        "id": "778831d9-de53-4da4-8ade-e3dcbd2922b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of wines of a particular rating:\n",
            "quality\n",
            "6    2198\n",
            "5    1457\n",
            "7     880\n",
            "8     175\n",
            "4     163\n",
            "3      20\n",
            "9       5\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[<Axes: title={'center': 'quality'}>]], dtype=object)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Get numeric distributions of our quality output\n",
        "print(\"Number of wines of a particular rating:\")\n",
        "counts = df['quality'].value_counts()\n",
        "print(counts)\n",
        "\n",
        "# Let's do a histogram plot. To do that we need to specify the\n",
        "# y-axis that we want to plot – i.e. 'quality' – and number of bins\n",
        "df.hist(column = 'quality', bins = len(counts))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1904d12-8911-4ea4-a4dc-5cb3157a0fd1",
      "metadata": {
        "id": "c1904d12-8911-4ea4-a4dc-5cb3157a0fd1"
      },
      "source": [
        "This looks somewhat normally distributed.  Let's say we care about good wines.  Then we'll just concentrate on differentiating great wines from the rest. Our task is now a classification. As we can see from the histogram below, the two classes are heavily imbalanced -- we have far more \"Not Good\" wines than the \"Good\" wines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9aebf57c-d97c-469b-9bd5-68f660902706",
      "metadata": {
        "id": "9aebf57c-d97c-469b-9bd5-68f660902706"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Axes: title={'center': 'Distribution of classes'}, xlabel='good wine'>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create a new column called good wine, where the value of the quality is 7 or better.\n",
        "df['good wine'] = np.where(df['quality']>=7, \"Good\", \"Not Good\")\n",
        "\n",
        "# Then remove the quality column (why)\n",
        "df.drop('quality', axis=1, inplace=True)\n",
        "\n",
        "df['good wine'].value_counts().plot(kind = 'bar', title = 'Distribution of classes')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e34552f-c406-440a-a3fa-86e9337e7013",
      "metadata": {
        "id": "2e34552f-c406-440a-a3fa-86e9337e7013"
      },
      "source": [
        "### .b Split dataset into train and test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd2e9a7f-21ce-461a-884e-519bfb045f8a",
      "metadata": {
        "id": "fd2e9a7f-21ce-461a-884e-519bfb045f8a"
      },
      "source": [
        "Let's split the data into training and testing data sets before we do anything else.   It's important to look only at the training data to develop our intuitions (**why?**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1d8492c0-aea9-44ce-8f74-38bd2f24b038",
      "metadata": {
        "id": "1d8492c0-aea9-44ce-8f74-38bd2f24b038"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training instances:  3428 \n",
            "Number of test instances:  1470\n"
          ]
        }
      ],
      "source": [
        "# Learn how to split test and training data from a whole\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Partion the features from the class to predict\n",
        "df_X = df[df.columns[df.columns != 'good wine']].copy() # get columns that are not 'good wine'\n",
        "df_y = df['good wine'].copy() # get the column named 'good wine'; this is our label\n",
        "\n",
        "# (random_state): we use a fixed random seed so we get the same results every time.\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.3, random_state=1)\n",
        "\n",
        "print (\"Number of training instances: \", len(X_train), \"\\nNumber of test instances: \", len(X_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b988fbc-d3ec-492b-b314-46a8306d2040",
      "metadata": {
        "id": "0b988fbc-d3ec-492b-b314-46a8306d2040"
      },
      "source": [
        "Let's look at the first few rows of the training portion of the dataset.  Understanding the data is **always** important in trying to build any model for prediction.\n",
        "You can check against the description of the dataset which is here: https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality.names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "fc01a3e1-8fc8-4bec-b574-8bfbc930ffbc",
      "metadata": {
        "id": "fc01a3e1-8fc8-4bec-b574-8bfbc930ffbc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4554</th>\n",
              "      <td>6.0</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.34</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.025</td>\n",
              "      <td>23.0</td>\n",
              "      <td>111.0</td>\n",
              "      <td>0.98961</td>\n",
              "      <td>3.36</td>\n",
              "      <td>0.37</td>\n",
              "      <td>12.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3401</th>\n",
              "      <td>8.8</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.30</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.028</td>\n",
              "      <td>34.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>0.99242</td>\n",
              "      <td>2.94</td>\n",
              "      <td>0.47</td>\n",
              "      <td>11.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3330</th>\n",
              "      <td>6.7</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.33</td>\n",
              "      <td>8.1</td>\n",
              "      <td>0.048</td>\n",
              "      <td>45.0</td>\n",
              "      <td>176.0</td>\n",
              "      <td>0.99472</td>\n",
              "      <td>3.11</td>\n",
              "      <td>0.52</td>\n",
              "      <td>10.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4462</th>\n",
              "      <td>7.1</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.20</td>\n",
              "      <td>2.8</td>\n",
              "      <td>0.038</td>\n",
              "      <td>28.0</td>\n",
              "      <td>109.0</td>\n",
              "      <td>0.98968</td>\n",
              "      <td>3.23</td>\n",
              "      <td>0.47</td>\n",
              "      <td>13.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3171</th>\n",
              "      <td>7.3</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.39</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.048</td>\n",
              "      <td>24.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>0.99044</td>\n",
              "      <td>2.94</td>\n",
              "      <td>0.35</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
              "4554            6.0              0.23         0.34             1.3      0.025   \n",
              "3401            8.8              0.19         0.30             5.0      0.028   \n",
              "3330            6.7              0.23         0.33             8.1      0.048   \n",
              "4462            7.1              0.42         0.20             2.8      0.038   \n",
              "3171            7.3              0.20         0.39             2.3      0.048   \n",
              "\n",
              "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
              "4554                 23.0                 111.0  0.98961  3.36       0.37   \n",
              "3401                 34.0                 120.0  0.99242  2.94       0.47   \n",
              "3330                 45.0                 176.0  0.99472  3.11       0.52   \n",
              "4462                 28.0                 109.0  0.98968  3.23       0.47   \n",
              "3171                 24.0                  87.0  0.99044  2.94       0.35   \n",
              "\n",
              "      alcohol  \n",
              "4554     12.7  \n",
              "3401     11.2  \n",
              "3330     10.1  \n",
              "4462     13.4  \n",
              "3171     12.0  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a4d9054-cb06-47e1-bf2c-22b3421a1485",
      "metadata": {
        "id": "5a4d9054-cb06-47e1-bf2c-22b3421a1485"
      },
      "source": [
        "We can also take a look at some general statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "72f10b76-0307-4e5b-9cd0-b4b2f05ef55c",
      "metadata": {
        "id": "72f10b76-0307-4e5b-9cd0-b4b2f05ef55c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>3428.000000</td>\n",
              "      <td>3428.000000</td>\n",
              "      <td>3428.000000</td>\n",
              "      <td>3428.000000</td>\n",
              "      <td>3428.000000</td>\n",
              "      <td>3428.000000</td>\n",
              "      <td>3428.000000</td>\n",
              "      <td>3428.000000</td>\n",
              "      <td>3428.000000</td>\n",
              "      <td>3428.000000</td>\n",
              "      <td>3428.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>6.852728</td>\n",
              "      <td>0.278197</td>\n",
              "      <td>0.334387</td>\n",
              "      <td>6.392255</td>\n",
              "      <td>0.045682</td>\n",
              "      <td>35.408693</td>\n",
              "      <td>138.459160</td>\n",
              "      <td>0.994048</td>\n",
              "      <td>3.185656</td>\n",
              "      <td>0.491362</td>\n",
              "      <td>10.488486</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.834514</td>\n",
              "      <td>0.101933</td>\n",
              "      <td>0.122178</td>\n",
              "      <td>5.077153</td>\n",
              "      <td>0.021071</td>\n",
              "      <td>17.272270</td>\n",
              "      <td>42.755363</td>\n",
              "      <td>0.003001</td>\n",
              "      <td>0.151288</td>\n",
              "      <td>0.113687</td>\n",
              "      <td>1.218602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>3.800000</td>\n",
              "      <td>0.080000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.012000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>0.987130</td>\n",
              "      <td>2.740000</td>\n",
              "      <td>0.230000</td>\n",
              "      <td>8.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>6.300000</td>\n",
              "      <td>0.210000</td>\n",
              "      <td>0.260000</td>\n",
              "      <td>1.700000</td>\n",
              "      <td>0.036000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>108.000000</td>\n",
              "      <td>0.991750</td>\n",
              "      <td>3.080000</td>\n",
              "      <td>0.410000</td>\n",
              "      <td>9.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>6.800000</td>\n",
              "      <td>0.260000</td>\n",
              "      <td>0.320000</td>\n",
              "      <td>5.200000</td>\n",
              "      <td>0.043000</td>\n",
              "      <td>34.000000</td>\n",
              "      <td>134.000000</td>\n",
              "      <td>0.993735</td>\n",
              "      <td>3.180000</td>\n",
              "      <td>0.480000</td>\n",
              "      <td>10.300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>7.300000</td>\n",
              "      <td>0.320000</td>\n",
              "      <td>0.390000</td>\n",
              "      <td>9.900000</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>46.000000</td>\n",
              "      <td>168.000000</td>\n",
              "      <td>0.996120</td>\n",
              "      <td>3.280000</td>\n",
              "      <td>0.550000</td>\n",
              "      <td>11.341667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>11.800000</td>\n",
              "      <td>1.100000</td>\n",
              "      <td>1.230000</td>\n",
              "      <td>65.800000</td>\n",
              "      <td>0.346000</td>\n",
              "      <td>289.000000</td>\n",
              "      <td>440.000000</td>\n",
              "      <td>1.038980</td>\n",
              "      <td>3.810000</td>\n",
              "      <td>1.080000</td>\n",
              "      <td>14.200000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
              "count    3428.000000       3428.000000  3428.000000     3428.000000   \n",
              "mean        6.852728          0.278197     0.334387        6.392255   \n",
              "std         0.834514          0.101933     0.122178        5.077153   \n",
              "min         3.800000          0.080000     0.000000        0.600000   \n",
              "25%         6.300000          0.210000     0.260000        1.700000   \n",
              "50%         6.800000          0.260000     0.320000        5.200000   \n",
              "75%         7.300000          0.320000     0.390000        9.900000   \n",
              "max        11.800000          1.100000     1.230000       65.800000   \n",
              "\n",
              "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
              "count  3428.000000          3428.000000           3428.000000  3428.000000   \n",
              "mean      0.045682            35.408693            138.459160     0.994048   \n",
              "std       0.021071            17.272270             42.755363     0.003001   \n",
              "min       0.012000             3.000000             18.000000     0.987130   \n",
              "25%       0.036000            23.000000            108.000000     0.991750   \n",
              "50%       0.043000            34.000000            134.000000     0.993735   \n",
              "75%       0.050000            46.000000            168.000000     0.996120   \n",
              "max       0.346000           289.000000            440.000000     1.038980   \n",
              "\n",
              "                pH    sulphates      alcohol  \n",
              "count  3428.000000  3428.000000  3428.000000  \n",
              "mean      3.185656     0.491362    10.488486  \n",
              "std       0.151288     0.113687     1.218602  \n",
              "min       2.740000     0.230000     8.000000  \n",
              "25%       3.080000     0.410000     9.400000  \n",
              "50%       3.180000     0.480000    10.300000  \n",
              "75%       3.280000     0.550000    11.341667  \n",
              "max       3.810000     1.080000    14.200000  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad0e0a49-c2a0-490d-970a-71eeebb6ed58",
      "metadata": {
        "id": "ad0e0a49-c2a0-490d-970a-71eeebb6ed58"
      },
      "source": [
        "### .c Train and Test the models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbc9decb-8422-4740-8301-ca2bec8c4b06",
      "metadata": {
        "id": "cbc9decb-8422-4740-8301-ca2bec8c4b06"
      },
      "source": [
        "Actually all of the above was just preparation. Now we have some intution, let's try fitting the dataset on both $k$ - NN and Decision Tree models using sklearn's API library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "98efb8fa-3bca-4734-8f71-d6f37ddde246",
      "metadata": {
        "id": "98efb8fa-3bca-4734-8f71-d6f37ddde246"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "kNN accuracy for training set: 1.000000\n",
            "kNN accuracy for test set: 0.794558\n"
          ]
        }
      ],
      "source": [
        "# Get the machine learning algorithm k-NN\n",
        "from sklearn import neighbors\n",
        "\n",
        "knn = neighbors.KNeighborsClassifier(n_neighbors = 1, metric='euclidean')\n",
        "knn_model = knn.fit(X_train, y_train)\n",
        "print('kNN accuracy for training set: %f' % knn_model.score(X_train, y_train))\n",
        "print('kNN accuracy for test set: %f' % knn_model.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "2aa8375a-a076-49a9-b182-d43bb1247288",
      "metadata": {
        "id": "2aa8375a-a076-49a9-b182-d43bb1247288"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decision Tree accuracy for training set: 1.000000\n",
            "Decision Tree accuracy for test set: 0.819048\n"
          ]
        }
      ],
      "source": [
        "# Get the machine learning algorithm Naïve Bayes\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Fix the random seed for decision tree classifier\n",
        "np.random.seed(seed=0)\n",
        "dt = DecisionTreeClassifier(max_depth=None)\n",
        "dt_model = dt.fit(X_train,y_train)\n",
        "print('Decision Tree accuracy for training set: %f' % dt_model.score(X_train, y_train))\n",
        "print('Decision Tree accuracy for test set: %f' % dt_model.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e1d46fa-3e2e-4e5a-a908-25b046f8331e",
      "metadata": {
        "id": "6e1d46fa-3e2e-4e5a-a908-25b046f8331e"
      },
      "source": [
        "After running these experiments you should have been able to get about 79% accuracy (on test set) for the nearest neighbor code and about 82% accuracy (on test set) using the Decision Tree algorithm.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CDmjJP10R4Kd",
      "metadata": {
        "id": "CDmjJP10R4Kd"
      },
      "source": [
        "That was easy! But it is important to know how to master these models yourself, so you're going to implement from scratch.  You'll practice this in the next sections."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13c6f7ac-1bcc-4e8a-919c-73650f0225f2",
      "metadata": {
        "id": "13c6f7ac-1bcc-4e8a-919c-73650f0225f2"
      },
      "source": [
        "To ease the implementation in the next section, we'll first transform the data into `numpy` arrays and transform the labels into lists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "71c9452f-69c1-4d43-8d8f-729ffed05b7f",
      "metadata": {
        "id": "71c9452f-69c1-4d43-8d8f-729ffed05b7f"
      },
      "outputs": [],
      "source": [
        "X_train, X_test = X_train.to_numpy(), X_test.to_numpy()\n",
        "y_train, y_test = y_train.to_list(), y_test.to_list()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7f64225-4087-4081-ac7d-641b4718be7d",
      "metadata": {
        "id": "c7f64225-4087-4081-ac7d-641b4718be7d"
      },
      "source": [
        "## 2. Programming : Implement $k$-Nearest Neighbor (kNN)\n",
        "\n",
        "We are going to implement a kNN to predict whether the wine is \"Good\" or \"Not good\" (so it is a binary or 2-class classification)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b20606f9-5045-43b4-ac4f-7fd1c6829823",
      "metadata": {
        "id": "b20606f9-5045-43b4-ac4f-7fd1c6829823"
      },
      "source": [
        "Next we are going to build a list of helper functions to help build the kNN model. Your task is to implement these functions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b94665f-50da-4bad-9544-36845fbc8aa3",
      "metadata": {
        "id": "3b94665f-50da-4bad-9544-36845fbc8aa3"
      },
      "source": [
        "### .a Calculate Manhattan_distance"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fc1819f-fb15-4250-b763-b38cfa1c1221",
      "metadata": {
        "id": "2fc1819f-fb15-4250-b763-b38cfa1c1221"
      },
      "source": [
        "**Manhattan Distance**: Manhattan Distance measures the sum of the absolute differences of two vectors' Cartesian coordinates.\n",
        "\n",
        "$$\n",
        "L_1(a, b) = |a_1-b_1| + |a_2-b_2| + \\cdots + |a_n-b_n|\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9356fc39-5fb8-4b51-ad0c-e30c5c5b1aa1",
      "metadata": {
        "id": "9356fc39-5fb8-4b51-ad0c-e30c5c5b1aa1"
      },
      "source": [
        "**Your Turn (Question 1):** Complete the code below to calculate the Manhattan Distance between two data points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "8c07a2c5-69f8-4f69-b1bb-205c6e1e5176",
      "metadata": {
        "id": "8c07a2c5-69f8-4f69-b1bb-205c6e1e5176"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def manhattan_distance(a, b):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      a (D) : a data point in numpy array of dimension D\n",
        "      b (D) : a data point in numpy array of dimension D\n",
        "    Returns:\n",
        "      dis(float): the Manhattan distance between the two data points\n",
        "    \"\"\"\n",
        "    dis = 0\n",
        "    ###########################\n",
        "  \n",
        "    # Calculate the element-wise absolute difference\n",
        "    c = np.abs(a - b)\n",
        "    \n",
        "    # Sum all elements in the difference vector\n",
        "    dis = np.sum(c)\n",
        "    ###########################\n",
        "    return dis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a522637b",
      "metadata": {
        "id": "a522637b"
      },
      "source": [
        "### .b Calculate Euclidean_distance"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "377609cc",
      "metadata": {
        "id": "377609cc"
      },
      "source": [
        "**Euclidean Distance**: Euclidean Distance measures the length of the line segment bewteen two points in the Euclidean space.\n",
        "\n",
        "$$\n",
        "L_2(a, b) = \\sqrt{(a_1-b_1)^2 + (a_2-b_2)^2 + \\cdots + (a_n-b_n)^2}\n",
        "$$\n",
        "\n",
        "Compared to the definition of Manhattan distance, it is natural to generalize the Euclidean Distance and Manhattan Distance into Minkowski Distance.\n",
        "\n",
        "**Minkowski Distance**: Minkowski Distance measures the p-order distance between two points. It is obvious that $p=1$ refers to Manhattan Distance while $p=2$ refers to Euclidean Distance.\n",
        "\n",
        "$$\n",
        "L_p(a, b) = (|a_1-b_1|^p + |a_2-b_2|^p + \\cdots + |a_n-b_n|^p)^{\\frac{1}{p}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b84f063",
      "metadata": {
        "id": "1b84f063"
      },
      "source": [
        "**Your Turn (Question 2):** Complete the code below to calculate the Minkowski Distance between two data points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "8655fcb1",
      "metadata": {
        "id": "8655fcb1"
      },
      "outputs": [],
      "source": [
        "def minkowski_distance(a, b, p):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      a (D) : a data point in numpy array of dimension D\n",
        "      b (D) : a data point in numpy array of dimension D\n",
        "      p     : distance order\n",
        "    Returns:\n",
        "      dis(float): the Minkowski distance between the two data points\n",
        "    \"\"\"\n",
        "    dis = 0\n",
        "    ###########################\n",
        "  \n",
        "    # Calculate the element-wise difference\n",
        "    c = np.abs(a - b)\n",
        "    \n",
        "    # Calculate the p-norm of the difference vector\n",
        "    dis = np.linalg.norm(c, ord=p)\n",
        "    ###########################\n",
        "    return dis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HKrVZUZEAcTx",
      "metadata": {
        "id": "HKrVZUZEAcTx"
      },
      "source": [
        "**Testing**: test to see if the Minkowski distance is consistent with your manual calculation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "9u-RXoP2Bx8v",
      "metadata": {
        "id": "9u-RXoP2Bx8v"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Minkowski Distance (p=1) between (1, 2, 3) and (0, 0, 0): 6.0\n",
            "Minkowski Distance (p=2) between (1, 2, 3) and (0, 0, 0): 3.7416573867739413\n",
            "------------------------------------------------------------------\n",
            "Minkowski Distance (p=1) between (100, 20, 30) and (0, 0, 0): 150.0\n",
            "Minkowski Distance (p=2) between (100, 20, 30) and (0, 0, 0): 106.30145812734649\n"
          ]
        }
      ],
      "source": [
        "x1 = np.asarray([1, 2, 3])\n",
        "x2 = np.asarray([0, 0, 0])\n",
        "print('Minkowski Distance (p=1) between (1, 2, 3) and (0, 0, 0):', minkowski_distance(x1, x2, p=1))\n",
        "print('Minkowski Distance (p=2) between (1, 2, 3) and (0, 0, 0):', minkowski_distance(x1, x2, p=2))\n",
        "\n",
        "print('------------------------------------------------------------------')\n",
        "x1 = np.asarray([100, 20, 30])\n",
        "x2 = np.asarray([0, 0, 0])\n",
        "print('Minkowski Distance (p=1) between (100, 20, 30) and (0, 0, 0):', minkowski_distance(x1, x2, p=1))\n",
        "print('Minkowski Distance (p=2) between (100, 20, 30) and (0, 0, 0):', minkowski_distance(x1, x2, p=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sMP2yG7DT8Gn",
      "metadata": {
        "id": "sMP2yG7DT8Gn"
      },
      "source": [
        "From the tests above, we can see that the Minkowski Distance (p=2) is closer to the dimension in which the two vectors have the largest difference, *e.g.*, $(100-0)$, $(3-0)$ than Minkowski Distance (p=1). **Why?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43359e84",
      "metadata": {
        "id": "43359e84"
      },
      "source": [
        "**Testing**: test to see if the Minkowski distance (p=1) is consistent with Manhattan distance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "b9da0eba",
      "metadata": {
        "id": "b9da0eba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Manhattan Distance between (1.5, 2.5, 3.5) and (0., 0., 0.): 7.5\n",
            "Minkowski Distance (p=1) between (1.5, 2.5, 3.5) and (0., 0., 0.): 7.5\n",
            "Manhattan Distance between (10., 20., 30.) and (13., 4., 7.): 42.0\n",
            "Minkowski Distance (p=1) between (10., 20., 30.) and (13., 4., 7.): 42.0\n"
          ]
        }
      ],
      "source": [
        "x1 = np.asarray([1.5, 2.5, 3.5])\n",
        "x2 = np.asarray([0., 0., 0.])\n",
        "print('Manhattan Distance between (1.5, 2.5, 3.5) and (0., 0., 0.):', manhattan_distance(x1, x2))\n",
        "print('Minkowski Distance (p=1) between (1.5, 2.5, 3.5) and (0., 0., 0.):', minkowski_distance(x1, x2, p=1))\n",
        "\n",
        "x1 = np.asarray([10., 20., 30.])\n",
        "x2 = np.asarray([13., 4., 7.])\n",
        "print('Manhattan Distance between (10., 20., 30.) and (13., 4., 7.):', manhattan_distance(x1, x2))\n",
        "print('Minkowski Distance (p=1) between (10., 20., 30.) and (13., 4., 7.):', minkowski_distance(x1, x2, p=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15668366",
      "metadata": {
        "id": "15668366"
      },
      "source": [
        "Since we can use minkowski_distance to calculate both Manhattan Distance and Euclidean Distance, we will keep using minkowski_distance function instead of manhattan_distance in the following programming.\n",
        "\n",
        "**Optional**: test to see what happened when p > 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "CmfD_TS4A6a3",
      "metadata": {
        "id": "CmfD_TS4A6a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "84.44345780422779\n",
            "99.69812000000002\n"
          ]
        }
      ],
      "source": [
        "# Testing: This should return 84.44345780422779\n",
        "print(minkowski_distance(X_test[0], X_train[0], p=2))\n",
        "\n",
        "# Testing: This should return 99.69812000000002\n",
        "print(minkowski_distance(X_test[0], X_train[0], p=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec1e5566-f7bd-4ba7-9882-932fc27e26d7",
      "metadata": {
        "id": "ec1e5566-f7bd-4ba7-9882-932fc27e26d7"
      },
      "source": [
        "### .c Find k Nearest Neighbors"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13eea814-4b5c-4437-bd63-27b508fcc89e",
      "metadata": {
        "id": "13eea814-4b5c-4437-bd63-27b508fcc89e"
      },
      "source": [
        "**$k$-NN function**: The $k$-NN function will find the k nearest data points given an array of distances. We want to return the corresponding labels of the k Nearest Neighbors."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d41ac3ab-2523-4992-9f29-fa8e6b18610c",
      "metadata": {
        "id": "d41ac3ab-2523-4992-9f29-fa8e6b18610c"
      },
      "source": [
        "**Your Turn (Question 3):** Complete the code below to find out the kNN's labels with the given array of distances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "603a33b7-431f-4b37-b685-08e5556e578b",
      "metadata": {
        "id": "603a33b7-431f-4b37-b685-08e5556e578b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2 1 0 1 2]\n"
          ]
        }
      ],
      "source": [
        "def find_kNN_labels(distances, labels, k):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      distances (m,) : a numpy array of dimension m that contains the distances between the test data point and all training data points\n",
        "      labels (m,) : a list of length m that contains the labels of all training data points\n",
        "      k: the number of nearest neighbors\n",
        "    Returns:\n",
        "      knn_labels (k,): the labels of the k nearest neighbors\n",
        "    \"\"\"\n",
        "\n",
        "    knn_labels = []\n",
        "    ###########################\n",
        "    # Get the indices that would sort the distances array\n",
        "    sorted_indices = np.argsort(distances)\n",
        "    \n",
        "    # Select the first k indices\n",
        "    k_indices = sorted_indices[:k]\n",
        "    \n",
        "    # Convert labels to a numpy array for advanced indexing\n",
        "    labels = np.array(labels)\n",
        "    \n",
        "    # Get the corresponding labels\n",
        "    knn_labels = labels[k_indices]\n",
        "    ###########################\n",
        "    return knn_labels\n",
        "\n",
        "# Testing: This should return [2, 1, 0, 1, 2]\n",
        "sample_distances = np.array([1.97, 1.94, 0.16, 0.91, 2.05, 1.5, 1.86, 2.01, 1.9, 1.67, 1.9, 1.14, 2.1, 1.94, 2.68, 0.08, 3.98, 3.05, 1.59, 2.4])\n",
        "sample_labels = np.array([0, 0, 1, 0, 1, 2, 1, 2, 1, 0, 1, 1, 0, 2, 1, 2, 0, 2, 2, 1])\n",
        "print(find_kNN_labels(sample_distances, sample_labels, 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2FbdUK94lZpp",
      "metadata": {
        "id": "2FbdUK94lZpp"
      },
      "source": [
        "### .d Find the Majority Class"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1P6SRD1Mmudn",
      "metadata": {
        "id": "1P6SRD1Mmudn"
      },
      "source": [
        "**Majority Class Function**: When finding the $k$-Nearest Neighbors, we return the value that represents the majority of the $k$ instances of the class as the answer."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Cs6Px7tkl-ud",
      "metadata": {
        "id": "Cs6Px7tkl-ud"
      },
      "source": [
        "**Your Turn (Question 4):** Complete the code below to find the majority class from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "ri7bAZaQlXTV",
      "metadata": {
        "id": "ri7bAZaQlXTV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Good\n"
          ]
        }
      ],
      "source": [
        "def get_majority_class(labels):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      labels(m): The corresponding labels of current sub-dataset\n",
        "          m: num_rows\n",
        "    Returns:\n",
        "      major: Type:String. The major class of this sub-dataset(e.g \"Good Wine\" or \"Not Good\")\n",
        "    \"\"\"\n",
        "    major = \"\"\n",
        "\n",
        "    # freq will store the number of occurences of the target labels\n",
        "    freq = {}\n",
        "    for entry in labels:\n",
        "        if (entry in freq):\n",
        "            freq[entry] += 1.0\n",
        "        else:\n",
        "            freq[entry] = 1.0\n",
        "\n",
        "    ###########################\n",
        "    #\n",
        "    # Your Turn (Q4): Write your code here\n",
        "    # Hint: Loop through each entry in labels, then find which entry occurs most frequently.\n",
        "\n",
        "    major = max(freq, key=freq.get)\n",
        "\n",
        "    ###########################\n",
        "    return major\n",
        "\n",
        "\n",
        "# Testing: This should return 'Good'\n",
        "sample_y = y_train[50:56]\n",
        "print(get_majority_class(sample_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "366e998f-51cf-43a9-a33a-bec29c01913a",
      "metadata": {
        "id": "366e998f-51cf-43a9-a33a-bec29c01913a"
      },
      "source": [
        "### .e Run $k$-NN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69143bc8-f0ba-41b5-ae1f-341746ef9910",
      "metadata": {
        "id": "69143bc8-f0ba-41b5-ae1f-341746ef9910"
      },
      "source": [
        "The following code `run_knn` is provided to you to run your kNN helper functions.\n",
        "\n",
        "You do not have to understand its code for the purpose of this exercise. Just run it and check the accuracy.\n",
        "\n",
        "Note that because the prediction of a single data points requires traversing the whole training set. The code is a bit slow. It can take $2\\sim 4$ minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "bc244881-0caa-41c1-a237-e6ebf10e93ff",
      "metadata": {
        "id": "bc244881-0caa-41c1-a237-e6ebf10e93ff"
      },
      "outputs": [],
      "source": [
        "def run_knn(distance_metric_function, **kwargs):\n",
        "    k = 1\n",
        "    correct = 0\n",
        "    for test_entry, label in zip(X_test, y_test):\n",
        "\n",
        "        ## find out the distance between the test data point and all training data points\n",
        "        distances = []\n",
        "        for train_entry in X_train:\n",
        "            distances.append(distance_metric_function(test_entry, train_entry, **kwargs))\n",
        "\n",
        "        knn_labels = find_kNN_labels(distances, y_train, k)\n",
        "        prediction = get_majority_class(knn_labels)\n",
        "\n",
        "        if prediction == label:\n",
        "            correct += 1\n",
        "\n",
        "    accuracy = correct / len(y_test)\n",
        "    print('Final accuracy')\n",
        "    print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cbc9fcf-ee0b-4654-b3a6-4cc41021130f",
      "metadata": {
        "id": "8cbc9fcf-ee0b-4654-b3a6-4cc41021130f"
      },
      "source": [
        "The following code runs $k$-NN with the Euclidean distance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "a069a328-adb9-42ea-bd3b-3499ea0a2132",
      "metadata": {
        "id": "a069a328-adb9-42ea-bd3b-3499ea0a2132"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final accuracy\n",
            "0.7945578231292517\n"
          ]
        }
      ],
      "source": [
        "## The accuracy should be 0.7945578231292517.\n",
        "kwargs = {}\n",
        "kwargs['p'] = 2\n",
        "run_knn(minkowski_distance, **kwargs) # note that minkowski_distance is a function, not a value"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3a2c83e-95a2-42a4-933f-311d524a80ac",
      "metadata": {
        "id": "f3a2c83e-95a2-42a4-933f-311d524a80ac"
      },
      "source": [
        "The following code runs $k$-NN with the Manhattan distance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "9d81df58",
      "metadata": {
        "id": "9d81df58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final accuracy\n",
            "0.8020408163265306\n"
          ]
        }
      ],
      "source": [
        "## The accuracy should be 0.8020408163265306\n",
        "kwargs = {}\n",
        "kwargs['p'] = 1\n",
        "run_knn(minkowski_distance, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae64e6d7",
      "metadata": {
        "id": "ae64e6d7"
      },
      "source": [
        "**Optional**: The following code runs $k$-NN with the Minkowski distance (p=10)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "858f4d42-9a0c-448b-8440-b8c3c8927853",
      "metadata": {
        "id": "858f4d42-9a0c-448b-8440-b8c3c8927853"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final accuracy\n",
            "0.7925170068027211\n"
          ]
        }
      ],
      "source": [
        "## The accuracy should be 0.7925170068027211\n",
        "kwargs = {}\n",
        "kwargs['p'] = 10\n",
        "run_knn(minkowski_distance, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0048bf28-85ef-4f17-91df-e5bd34fd7ab5",
      "metadata": {
        "id": "0048bf28-85ef-4f17-91df-e5bd34fd7ab5"
      },
      "source": [
        "## 3. Programming : Implement Decision Tree\n",
        "\n",
        "We are going to implement a decision tree to predict whether the wine is \"Good\" or \"Not good\" (again, it is a 2-class classification)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f497ee5d-968f-4554-b9d7-e1307c990ec3",
      "metadata": {
        "id": "f497ee5d-968f-4554-b9d7-e1307c990ec3"
      },
      "source": [
        "### .a Discretize the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cee51a3-b13f-4f64-a479-2202cbea65b8",
      "metadata": {
        "id": "5cee51a3-b13f-4f64-a479-2202cbea65b8"
      },
      "source": [
        "We are going to make use of 11 _attributes_ (also known as _features_ or _input dimensions_, but for this decision trees to be consistent with the algorithm, we'll use \"attributes\").\n",
        "\n",
        "In this assignment, we will implement a basic version of Decision tree that takes in categorical attributes. Thus, we are going to discretize the continuous features, where values **larger** than the mean is assigned **1** and values **smaller** than the mean is assigned **0**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "52c8a15a-091e-40e7-b85f-61a8e79e052e",
      "metadata": {
        "id": "52c8a15a-091e-40e7-b85f-61a8e79e052e"
      },
      "outputs": [],
      "source": [
        "def discretize_data(train_data, test_data):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        train_data (np array): the training set\n",
        "        test_data (np array): the test set\n",
        "\n",
        "    Returns:\n",
        "    np_train_data (np array): contains the discretized training set\n",
        "    np_test_data (np array): contains the discretized test set\n",
        "    \"\"\"\n",
        "    train_data_discrete = np.zeros_like(train_data) # initialize\n",
        "    test_data_discrete = np.zeros_like(test_data)\n",
        "\n",
        "    D = train_data_discrete.shape[1]\n",
        "\n",
        "    for i in range(D):\n",
        "        mean_value = np.mean(train_data[:, i]) # get mean values at which to label as 1 (larger) or 0 (smaller)\n",
        "        train_data_discrete[:, i] = (train_data[:, i] > mean_value).astype(float)\n",
        "        test_data_discrete[:, i] = (test_data[:, i] > mean_value).astype(float)\n",
        "\n",
        "    return train_data_discrete, test_data_discrete\n",
        "\n",
        "X_train_discrete, X_test_discrete = discretize_data(X_train, X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "06203550-afe2-4496-b0b8-53da94f6b5c1",
      "metadata": {
        "id": "06203550-afe2-4496-b0b8-53da94f6b5c1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([[1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1.],\n",
              "        [0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1.],\n",
              "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "        [1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0.],\n",
              "        [0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "        [1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1.],\n",
              "        [0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
              "        [0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1.],\n",
              "        [1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0.]]),\n",
              " ['Not Good',\n",
              "  'Good',\n",
              "  'Not Good',\n",
              "  'Not Good',\n",
              "  'Not Good',\n",
              "  'Not Good',\n",
              "  'Good',\n",
              "  'Good',\n",
              "  'Good',\n",
              "  'Not Good'])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample_X = X_train_discrete[10:20]\n",
        "sample_y = y_train[10:20]\n",
        "sample_X, sample_y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "193e7af2-57e0-494a-b1fe-ce16348f05ea",
      "metadata": {
        "id": "193e7af2-57e0-494a-b1fe-ce16348f05ea"
      },
      "source": [
        "### .b Choose the best Feature based on Majority"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08aa95d2-a41a-4288-9cbc-f5fd38afccc1",
      "metadata": {
        "id": "08aa95d2-a41a-4288-9cbc-f5fd38afccc1"
      },
      "source": [
        "**Majority Class Function**: When you reach a terminal node (leaf) in a decision tree, we return the label that represents the majority of the sub-dataset's instances of the class as the answer."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Bv2g3fB_mCQS",
      "metadata": {
        "id": "Bv2g3fB_mCQS"
      },
      "source": [
        "We can reuse the `get_majority_class` function from the implementation of $k$-Nearest Neighbors"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "732bc81a-6c81-4d82-94a1-f64ac5ca4eed",
      "metadata": {
        "id": "732bc81a-6c81-4d82-94a1-f64ac5ca4eed"
      },
      "source": [
        "### .c Calculate Entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bd7f40c-8263-429c-b2ad-71d0d14dc61f",
      "metadata": {
        "id": "5bd7f40c-8263-429c-b2ad-71d0d14dc61f"
      },
      "source": [
        "**Entropy Function**: Calculate the entropy of this current sub-dataset:\n",
        "\n",
        "Recall: $H(X) = - \\sum_{i=0}^C p_i\\log_2(p_i)$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f95fbba7-c7eb-4a60-a123-38f109edb54b",
      "metadata": {
        "id": "f95fbba7-c7eb-4a60-a123-38f109edb54b"
      },
      "source": [
        "**Your Turn (Question 5):** Complete the code below to calculate entropy of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "3426b56e-ab12-416f-b9b0-dd91099d828e",
      "metadata": {
        "id": "3426b56e-ab12-416f-b9b0-dd91099d828e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9709505944546686\n"
          ]
        }
      ],
      "source": [
        "def entropy(labels):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      labels(m): The corresponding labels of current sub-dataset of the sub-tree\n",
        "          m: num_rows\n",
        "    Returns:\n",
        "      dataEntropy: The entropy of this current sub-dataset\n",
        "    \"\"\"\n",
        "    dataEntropy = 0.0\n",
        "    # freq will store the number of occurrences of the target labels\n",
        "    freq = {}\n",
        "    for entry in labels:\n",
        "        if (entry in freq):\n",
        "            freq[entry] += 1.0\n",
        "        else:\n",
        "            freq[entry] = 1.0\n",
        "\n",
        "    ###########################\n",
        "    #\n",
        "    # Your Turn (Q5): Write your code here\n",
        "    # Hint: Loop through each row of data, get the last column, find number of occurrences of each value, then use the above equation.\n",
        "    #\n",
        "\n",
        "    # Calculate the total number of labels\n",
        "    total_labels = len(labels)\n",
        "    \n",
        "    # Calculate the entropy\n",
        "    for key in freq:\n",
        "        prob = freq[key] / total_labels\n",
        "        dataEntropy -= prob * np.log2(prob)\n",
        "\n",
        "    ###########################\n",
        "\n",
        "    return dataEntropy\n",
        "\n",
        "# Testing: This should return 0.9709505944546686 (in log 2)\n",
        "print(entropy(sample_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43dcec20-82f1-4b7b-9500-313b9bbe8bc2",
      "metadata": {
        "id": "43dcec20-82f1-4b7b-9500-313b9bbe8bc2"
      },
      "source": [
        "### .d Calculate Information Gain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3c82aa8-3f79-4b57-a111-4ef1969eee7b",
      "metadata": {
        "id": "c3c82aa8-3f79-4b57-a111-4ef1969eee7b"
      },
      "source": [
        "**Information Gain**: The information gained by splitting the current (sub)-dataset using the attribute.\n",
        "\n",
        "Recall:  Information Gain is a metric that measures the expected reduction in the impurity of the collection $S$, caused by splitting the data according to any given attribute. A chosen attribute $x_i$ divides the example set S into subsets\n",
        "$S_1 , S_2 , ... , S_{C_i}$ according to the $C_i$ distinct values for $x_i$ .\n",
        "The entropy then reduces to the entropy of the subsets $S_1 , S_2 , ... , S_{C_i}$:\n",
        "\n",
        "<div align=\"center\">\n",
        "$\\text{remainder}(S, x_i) = \\sum_{j=1}^{C_i} \\frac{|S_j|}{|S|} H(S_j)$\n",
        "</div>\n",
        "\n",
        "The Information Gain (IG; “reduction in entropy”) from knowing the value of $x_i$ is:\n",
        "<div align=\"center\">\n",
        "$IG(S, x_i) = H(S) - \\text{remainder}(S, x_i) $  \n",
        "</div>\n",
        "\n",
        "Subsequently, we choose the attribute with the largest IG."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8541937a-d959-4e6d-84bc-9610b83daa66",
      "metadata": {
        "id": "8541937a-d959-4e6d-84bc-9610b83daa66"
      },
      "source": [
        "**Your Turn (Question 6):** Complete the code below to calculate information gain of a given attribute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "de08aa3c-16fa-498f-8b16-57d571126ad3",
      "metadata": {
        "id": "de08aa3c-16fa-498f-8b16-57d571126ad3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.01997309402197489\n"
          ]
        }
      ],
      "source": [
        "def info_gain(data, labels, attribute, attributes):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      data(m, D): The current sub-dataset of the sub-tree\n",
        "          m: num_rows\n",
        "          D: num_features\n",
        "      labels(m): The corresponding labels of current sub-dataset of the sub-tree\n",
        "          m: num_rows\n",
        "      attribute: The attribute used to split data\n",
        "      attributes: The list of current remaining attributes\n",
        "    Returns:\n",
        "      info_gain : information gain of the given dataset\n",
        "    \"\"\"\n",
        "\n",
        "    freq = {}\n",
        "    subsetEntropy = 0.0\n",
        "\n",
        "    # Get the column index of this attribute\n",
        "    i = attributes.index(attribute)\n",
        "\n",
        "    for entry in data:\n",
        "        if (entry[i] in freq):\n",
        "            freq[entry[i]] += 1.0\n",
        "        else:\n",
        "            freq[entry[i]]  = 1.0\n",
        "\n",
        "    ###########################\n",
        "    # Calculate the total number of entries\n",
        "    total_entries = len(data)\n",
        "\n",
        "    # Calculate the subset entropy\n",
        "    for value in freq:\n",
        "        prob = freq[value] / total_entries\n",
        "        subset_data = [data[j] for j in range(total_entries) if data[j][i] == value]\n",
        "        subset_labels = [labels[j] for j in range(total_entries) if data[j][i] == value]\n",
        "        subsetEntropy += prob * entropy(subset_labels)\n",
        "\n",
        "    ###########################\n",
        "    return (entropy(labels) - subsetEntropy)\n",
        "\n",
        "# Testing: This should return 0.01997309402197489\n",
        "attributes = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']\n",
        "print(info_gain(sample_X, sample_y, 'residual sugar', attributes))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49490e04-d33c-4f72-a6d4-dadcf2dcaf5b",
      "metadata": {
        "id": "49490e04-d33c-4f72-a6d4-dadcf2dcaf5b"
      },
      "source": [
        "### .e Find the Best Attribute"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "156d86b5-6fa4-43a1-a7e2-22c74a98d418",
      "metadata": {
        "id": "156d86b5-6fa4-43a1-a7e2-22c74a98d418"
      },
      "source": [
        "Now we will write a function to choose the **best** (most discriminating) attribute for a given data, here best indicates having **largest** information gain (IG) among all attributes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9b03b21-a83c-4b15-9eec-261c552af288",
      "metadata": {
        "id": "f9b03b21-a83c-4b15-9eec-261c552af288"
      },
      "source": [
        "**Your Turn (Question 7):** Complete the code below to get the best attribute based on information gain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "5fb2edb1-21b2-44bd-b3c8-cc700bfb6e8a",
      "metadata": {
        "id": "5fb2edb1-21b2-44bd-b3c8-cc700bfb6e8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "citric acid\n"
          ]
        }
      ],
      "source": [
        "def get_best_gain_attribute(data, labels, attributes):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      data(m, D): The current sub-dataset of the sub-tree\n",
        "          m: num_rows\n",
        "          D: num_features + 1 (last column is the label)\n",
        "      attributes: The list of current remaining attributes\n",
        "    Returns:\n",
        "      best: The best attribute to split based on info gain.\n",
        "    \"\"\"\n",
        "    best = attributes[0]\n",
        "    max_gain = -float('inf')  # Initialize to a very small value\n",
        "\n",
        "    for attr in attributes:\n",
        "        gain = info_gain(data, labels, attr, attributes)\n",
        "        if gain > max_gain:\n",
        "            max_gain = gain\n",
        "            best = attr\n",
        "    \n",
        "    return best\n",
        "\n",
        "# Testing: This should return 'citric acid'\n",
        "attributes = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']\n",
        "print(get_best_gain_attribute(sample_X, sample_y, attributes))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6aab711-569d-4ace-849c-0684a1e420f7",
      "metadata": {
        "id": "b6aab711-569d-4ace-849c-0684a1e420f7"
      },
      "source": [
        "### .f Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f15bcceb-dbe0-4f2e-99dc-c1da03572196",
      "metadata": {
        "id": "f15bcceb-dbe0-4f2e-99dc-c1da03572196"
      },
      "source": [
        "We will define two helper functions here. First, `get_unique_values` returns the unique values of a given attribute. The second function, `get_sub_data` returns the subset of rows (instances) containing a specific value of a chosen attribute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "2fc3c4a5-35ce-4c0b-893c-08171dd8ffe5",
      "metadata": {
        "id": "2fc3c4a5-35ce-4c0b-893c-08171dd8ffe5"
      },
      "outputs": [],
      "source": [
        "# These two functions are helper functions\n",
        "# This function will get unique values for that particular attribute from the given data\n",
        "def get_unique_values(data, attributes, attribute):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      data (m,D) : Current subset of data\n",
        "      attributes : The list of current remaining attributes\n",
        "      attribute : Our point of interest\n",
        "\n",
        "    Returns:\n",
        "      values : List of unique values for our point of interest\n",
        "    \"\"\"\n",
        "    index = attributes.index(attribute)\n",
        "    values = []\n",
        "    #\n",
        "    for entry in data:\n",
        "        if entry[index] not in values:\n",
        "            values.append(entry[index])\n",
        "\n",
        "    return values\n",
        "\n",
        "# This function will get all the rows of the data where the chosen \"best\" attribute has a value \"val\"\n",
        "def get_sub_data(data, labels, attributes, best, val):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      data (m,D) : Current subset of data\n",
        "      labels (m) : Corresponding labels of current subset of data\n",
        "      attributes : The list of current remaining attributes\n",
        "      best : The attribute of which data we will extract\n",
        "      val : We are interested only on this value of the `best` attribute\n",
        "    Returns:\n",
        "      new_data : Data subset containing only those rows where `best` attribute = val\n",
        "      new_labels : Label subset containing only those values where `best` attribute = val\n",
        "    \"\"\"\n",
        "    new_data = [[]]\n",
        "    new_labels = []\n",
        "    attribute_index = attributes.index(best)\n",
        "\n",
        "    for index, entry in enumerate(data):\n",
        "        if (entry[attribute_index] == val):\n",
        "            newEntry = []\n",
        "            for i in range(0,len(entry)):\n",
        "                if(i != attribute_index):\n",
        "                    newEntry.append(entry[i])\n",
        "            new_data.append(newEntry)\n",
        "            new_labels.append(labels[index])\n",
        "    new_data.remove([])\n",
        "    return new_data, new_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34bea7f2-dc88-4ef9-ada8-c0225dd3fa00",
      "metadata": {
        "id": "34bea7f2-dc88-4ef9-ada8-c0225dd3fa00"
      },
      "source": [
        "### .g Build the Decision Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a716217b-4105-4fb4-ab5f-40666d1d671e",
      "metadata": {
        "id": "a716217b-4105-4fb4-ab5f-40666d1d671e"
      },
      "source": [
        "In the below code, your task is to build a tree recursively. Starting at the root, pick the best attribute to split on, and call the `build_tree` function on each of the sub-trees.  Check the inlined code comments for clarification."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44185b20-9ad4-4afd-8a83-b63bd96d6e4a",
      "metadata": {
        "id": "44185b20-9ad4-4afd-8a83-b63bd96d6e4a"
      },
      "source": [
        "**Your Turn (Question 8):** Complete the code below to build the tree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "c5327d2e-0c60-41a1-a9f0-5c9870e50973",
      "metadata": {
        "id": "c5327d2e-0c60-41a1-a9f0-5c9870e50973"
      },
      "outputs": [],
      "source": [
        "def get_majority_class(labels):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      labels(m): The corresponding labels of current sub-dataset\n",
        "          m: num_rows\n",
        "    Returns:\n",
        "      major: Type:String. The major class of this sub-dataset(e.g \"Good Wine\" or \"Not Good\")\n",
        "    \"\"\"\n",
        "    freq = {}\n",
        "    for entry in labels:\n",
        "        if entry in freq:\n",
        "            freq[entry] += 1.0\n",
        "        else:\n",
        "            freq[entry] = 1.0\n",
        "\n",
        "    major = max(freq, key=freq.get)\n",
        "    return major\n",
        "\n",
        "def build_tree(data, labels, attributes, default):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        data(m, D): The current sub-dataset of the sub-tree\n",
        "            m: num_rows\n",
        "            D: num_features + 1 (last column is the label)\n",
        "        attributes: The list of current remaining attributes\n",
        "      Returns:\n",
        "        tree: The constructed tree as object. For example if the root is gender,\n",
        "              then a tree of depth 2 is like\n",
        "              {'gender': {'male': sub_tree1, 'female': sub_tree2}}\n",
        "    \"\"\"\n",
        "    # Base cases\n",
        "    if len(data) == 0:\n",
        "        return default\n",
        "    if len(set(labels)) == 1:\n",
        "        return labels[0]\n",
        "    if len(attributes) == 0:\n",
        "        return get_majority_class(labels)\n",
        "\n",
        "    # Recursive case\n",
        "    best_attr = get_best_gain_attribute(data, labels, attributes)\n",
        "    tree = {best_attr: {}}\n",
        "    best_attr_index = attributes.index(best_attr)\n",
        "\n",
        "    # Get unique values of the best attribute\n",
        "    unique_values = set([entry[best_attr_index] for entry in data])\n",
        "\n",
        "    for value in unique_values:\n",
        "        subset_data = [data[i] for i in range(len(data)) if data[i][best_attr_index] == value]\n",
        "        subset_labels = [labels[i] for i in range(len(labels)) if data[i][best_attr_index] == value]\n",
        "        subtree = build_tree(subset_data, subset_labels, [attr for attr in attributes if attr != best_attr], get_majority_class(labels))\n",
        "        tree[best_attr][value] = subtree\n",
        "\n",
        "    return tree"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac247c4f-5622-45da-8fb4-b48f1b9e95d2",
      "metadata": {
        "id": "ac247c4f-5622-45da-8fb4-b48f1b9e95d2"
      },
      "source": [
        "The below code block containing `run_decision_tree` does exactly as its name, and you don't have to understand its code for the purpose of this exercise. Just run it and check the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "16a807fb-aebf-4b14-8d49-3b629878e847",
      "metadata": {
        "id": "16a807fb-aebf-4b14-8d49-3b629878e847"
      },
      "outputs": [],
      "source": [
        "# Class Node which will be used while classifying a test instance using the tree built (\"fit\") earlier\n",
        "class Node():\n",
        "    value = \"\"\n",
        "    children = []\n",
        "\n",
        "    def __init__(self, val, dictionary):\n",
        "        self.value = val\n",
        "        if (isinstance(dictionary, dict)):\n",
        "            self.children = list(dictionary.keys())\n",
        "\n",
        "def run_decision_tree():\n",
        "    attributes = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']\n",
        "    tree = build_tree(X_train_discrete, y_train, attributes, get_majority_class(y_train))\n",
        "    results = []\n",
        "\n",
        "    for entry, label in zip(X_test_discrete, y_test):\n",
        "        tempDict = tree.copy()\n",
        "        result = \"\"\n",
        "        while(isinstance(tempDict, dict)):\n",
        "            root = Node(list(tempDict.keys())[0], tempDict[list(tempDict.keys())[0]])\n",
        "            tempDict = tempDict[list(tempDict.keys())[0]]\n",
        "            index = attributes.index(root.value)\n",
        "            value = entry[index]\n",
        "            if(value in list(tempDict.keys())):\n",
        "                child = Node(value, tempDict[value])\n",
        "                result = tempDict[value]\n",
        "                tempDict = tempDict[value]\n",
        "            else:\n",
        "                result = \"Null\"\n",
        "                break\n",
        "        if result != \"Null\":\n",
        "            results.append(result == label)\n",
        "\n",
        "    accuracy = float(results.count(True))/float(len(results))\n",
        "#     print(results)\n",
        "    print(\"FINAL ACCURACY: \")\n",
        "    print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "25076d80-85cc-47bb-b63f-c8956de81e4b",
      "metadata": {
        "id": "25076d80-85cc-47bb-b63f-c8956de81e4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FINAL ACCURACY: \n",
            "0.8212389380530973\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e574952-8958-4b4a-b1d9-65233d061840",
      "metadata": {
        "id": "4e574952-8958-4b4a-b1d9-65233d061840"
      },
      "source": [
        "## 4. Comparison between $k$-NN and Decision Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FQ85usDdFUbI",
      "metadata": {
        "id": "FQ85usDdFUbI"
      },
      "source": [
        "The above is akin to a skeleton of a machine learning project.  Critical for understanding is knowing why the performance of a learner is as it is. As a scientist, you'll want to know why you think a learner performs well or poorly and **then** use experiments to verify your hunches.  To build up this skill we need to practice it, as you'll need to apply this in your own group projects later."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b610577-3690-403c-b18f-93214458bda2",
      "metadata": {
        "id": "7b610577-3690-403c-b18f-93214458bda2"
      },
      "source": [
        "**Your Turn (Question 9):** Compare the performances of our implementation of $k$-NN and decision tree. Tell us what you have observed and why one of the models performs better than the other.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gBNkBYnOo3ks",
      "metadata": {
        "id": "gBNkBYnOo3ks"
      },
      "source": [
        "**Your answer here (Question 9)**: _Write down your answer in the current cell.  A brief 1–3 sentence answer is sufficient._\n",
        "\n",
        "The decision tree model performs slightly better than the $k$-NN model on the test set, likely due to its ability to handle complex decision boundaries and better generalization. While both models perfectly fit the training data, the decision tree's hierarchical structure helps it generalize more effectively, resulting in higher test accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VGlAWahG2MW9",
      "metadata": {
        "id": "VGlAWahG2MW9"
      },
      "source": [
        "**Your Turn (Question 10):** Compare the running times of $k$-NN and decision tree. Briefly tell us why $k$-NN is much slower than the other."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gYlhuN-vpBTy",
      "metadata": {
        "id": "gYlhuN-vpBTy"
      },
      "source": [
        "**Your answer here (Question 10)**: _Write down your answer in the current cell.  A brief 1–3 sentence answer is sufficient._\n",
        "\n",
        "$k$-NN is much slower than decision trees because it requires computing the distance between the query instance and all instances in the training set for each prediction, leading to high computational complexity during the prediction phase. In contrast, decision trees have a higher computational cost during training but are very efficient during prediction, as they only need to traverse the tree from the root to a leaf node. This makes decision trees faster for predictions, especially on large datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fSVen-P92HPP",
      "metadata": {
        "id": "fSVen-P92HPP"
      },
      "source": [
        "**Your Turn (Question 11):** Compare the performances of your implementation of decision tree and sklearn's implementation. Which one has the better performance? Why do you think that is?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5W8zvwKapCvJ",
      "metadata": {
        "id": "5W8zvwKapCvJ"
      },
      "source": [
        "**Your answer here (Question 11)**: _Write down your answer in the current cell.  A brief 1–3 sentence answer is sufficient._\n",
        "\n",
        "The sklearn implementation of the decision tree generally has better performance compared to our custom implementation. This is because sklearn's implementation is highly optimized and includes various techniques to prevent overfitting, such as pruning and handling missing values. Additionally, sklearn leverages efficient algorithms and data structures, making it more robust and faster in both training and prediction phases."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "h0ySjsCG3r9j",
      "metadata": {
        "id": "h0ySjsCG3r9j"
      },
      "source": [
        "**Your Turn (Question 12):** Identify an instance where $k$-NN predicts a different label than Decision Tree. Briefly tell us why you think they predicted differently, and what can you do to make them agree more?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Z3c_dfKQ3sAy",
      "metadata": {
        "id": "Z3c_dfKQ3sAy"
      },
      "source": [
        "**Your answer here (Question 12)**: _Write down your answer in the current cell.  A brief 1–3 sentence answer is sufficient._\n",
        "\n",
        "An instance where $k$-NN predicts a different label than the Decision Tree could occur when the instance lies near the decision boundary. $k$-NN bases its prediction on the majority label of the nearest neighbors, which can be influenced by local variations and noise. In contrast, the Decision Tree uses a series of hierarchical rules that may not capture these local variations. To make them agree more, you can:\n",
        "\n",
        "- Increase the value of $k$ in $k$-NN to smooth out local variations.\n",
        "- Prune the Decision Tree to avoid overfitting and make it more generalizable.\n",
        "- Use ensemble methods like bagging or boosting to combine the strengths of both models and reduce their individual weaknesses."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "j5yziGEN4J1U",
      "metadata": {
        "id": "j5yziGEN4J1U"
      },
      "source": [
        "Congratulations 🎉 🎉 ! You have come to the end of the assignment.\n",
        "**Remember** to submit your \"Your Turn\" code, results, and answers to Canvas to be graded for your work."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
